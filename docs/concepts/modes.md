# Computation Modes âš™ï¸

TempoEval allows you to compute metrics in different ways depending on your resources (time, compute, money) and reliability needs.

## 1. Focus Time Mode (Recommended)

Uses set operations on extracted time intervals.

-   **Logic**: $Relevance = QFT \cap DFT \neq \emptyset$
-   **Pros**:
    -   âš¡ **Fast**: Millions of docs in seconds.
    -   ğŸ’¸ **Free**: No API calls (using Regex or pure Python temporal taggers).
    -   ğŸ”¬ **Interpretable**: You can see exactly which years matched.
    -   ğŸ”„ **Deterministic**: Same input always gives same score.
-   **Cons**:
    -   Requires accurate extraction.

```python
metric = TemporalPrecision(use_focus_time=True)
```

## 2. LLM-as-a-Judge Mode

Uses an LLM to judge each retrieved document individually.

-   **Logic**: Prompt LLM "Is this document temporally relevant to the query?"
-   **Pros**:
    -   ğŸ§  **Flexible**: Captures nuance extraction might miss.
    -   ğŸ› ï¸ **Easy Setup**: No regex tuning needed.
-   **Cons**:
    -   ğŸ”´ **Slow**: Latency scales linearly with $N_{docs}$.
    -   $$$ **Expensive**: Significant API costs for large benchmarks.
    -   ğŸ² **Non-deterministic**: Scores can drift.

```python
metric = TemporalPrecision(use_focus_time=False, use_llm=True)
metric.llm = provider
```

## 3. Gold Labels Mode

Standard Information Retrieval evaluation.

-   **Logic**: Compare retrieved IDs with human-labeled relevant IDs.
-   **Pros**:
    -   Industry Standard ($NDCG$, $MAP$).
-   **Cons**:
    -   âŒ **Binary**: Ignores *why* (temporal vs topical).
    -   ğŸ“‰ **Sparse**: Unlabeled relevant docs are counted as errors.

```python
metric = TemporalPrecision()
metric.compute(retrieved_ids=[...], gold_ids=[...])
```
